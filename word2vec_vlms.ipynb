{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNw6yAD1N2PfGzsCY2B4Mf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alasarerhan/Deep-Learning-Projects/blob/main/word2vec_vlms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Model Using Skip-Gram Architecture"
      ],
      "metadata": {
        "id": "qdEZiOC4AW5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "uBcCIiSMA2qG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "66-_40o_9hpX"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy requests tqdm torch scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure Reproducibility with Random Seeds\n",
        "\n",
        "Many of the algorithms used in this notebook, such as sub-sampling, negative sampling, and model initialization, rely on randomness to function. While this randomness can improve efficiency and generalization, it also means that running the same code multiple times may yield slightly different results.\n",
        "\n",
        "To ensure consistent and reproducible results across different runs on the notebook, we set random seeds for all random processes used in the code. This includes Python's random module, NumPy, and PyTorch. By doing so, we guarantee that operations like sampling, model initialization, and training yield the same results every time the notebook is executed."
      ],
      "metadata": {
        "id": "xTBrolBRBE9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "FHCW0Gc9BAjD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Runtime for GPU Acceleration\n",
        "\n",
        "Let's make sure that we have access to the GPU. We can use nvidia-smi command to do that."
      ],
      "metadata": {
        "id": "kzEmbkesCy94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVE74xuLCq2I",
        "outputId": "295ab7ca-b9b2-42fe-d228-4641408326d0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 28 19:25:13 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Prepare Dataset\n",
        "\n",
        "This section downloads the text8 dataset, a pre-processed collection of Wikipedia text commonly used for language modeling. The text8 dataset is already cleaned and formatted: it contains only lowercase alphabetic characters, with punctuation, numbers, and case distinctions removed. The dataset is tokenized into words, making it ready for vocabulary construction and subsequent preprocessing steps."
      ],
      "metadata": {
        "id": "iv0aumsuDGVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "URL = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "FILENAME = \"text8.zip\"\n",
        "\n",
        "if not os.path.isfile(FILENAME):\n",
        "    response = requests.get(URL, stream=True)\n",
        "    with open(FILENAME, 'wb') as file_obj:\n",
        "        for chunk in response.iter_content(chunk_size=1024):\n",
        "            if chunk:\n",
        "                file_obj.write(chunk)\n",
        "\n",
        "if not os.path.isfile(\"text8\"):\n",
        "    with zipfile.ZipFile(FILENAME, 'r') as zipped_file:\n",
        "        zipped_file.extractall(\".\")\n",
        "\n",
        "def load_text_file():\n",
        "    with open(\"text8\", \"r\", encoding=\"utf-8\") as file_obj:\n",
        "        text_data = file_obj.read()\n",
        "    return text_data.strip().split()\n",
        "\n",
        "words = load_text_file()\n",
        "print(words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLL0R3xrDDv5",
        "outputId": "a9cd9d5c-e6b2-4cf4-c51a-af4f074b8129"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of words in the text8 dataset: {len(words)}\")\n",
        "print(f\"Number of unique words in the text8 dataset: {len(set(words))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWUqQ73jD3s5",
        "outputId": "8a35262e-f20a-438a-bce4-58bce58dc316"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the text8 dataset: 17005207\n",
            "Number of unique words in the text8 dataset: 253854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary with Most Frequent Tokens\n",
        "\n",
        "This section constructs a vocabulary by retraining only words that appear at least a specified number of times. Words that do not meet this frequency threshold are discarded entirely, ensuring that the vocabulary reduces noise and memory consumption while aligning with the Word2Vec methodology."
      ],
      "metadata": {
        "id": "W1984LBrFJS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocabulary(words, min_frequency):\n",
        "    word_counter = Counter(words)\n",
        "    mapping = {}\n",
        "    for index, (word, count) in enumerate(word_counter.most_common()):\n",
        "        if count < min_frequency:\n",
        "            break\n",
        "        mapping[word] = index\n",
        "    return mapping"
      ],
      "metadata": {
        "id": "s8u0rT5MD2_r"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MIN_FREQUENCY = 10\n",
        "\n",
        "word_to_index = build_vocabulary(words, min_frequency=MIN_FREQUENCY)\n",
        "index_to_word = {val: key for key, val in word_to_index.items()}\n",
        "vocabluary_size = len(word_to_index)"
      ],
      "metadata": {
        "id": "t1lpfvf2GFSG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in words if w in word_to_index]\n",
        "print(f\"Number of words in the text8 dataset: {len(words)}\")\n",
        "print(f\"Number of unique words in the text8 dataset: {len(set(words))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7biSJmk1Gxwm",
        "outputId": "21808e95-5b65-46ea-be88-30bd4ca52d8a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the text8 dataset: 16561031\n",
            "Number of unique words in the text8 dataset: 47134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Sub-sampling to Reduce Frequent Words\n",
        "\n",
        "To mitigate the dominance of frequent tokens, a sub-sampling technique is applied. Tokens that appear excessively often are probabilistically downsampled, ensuring a balanced dataset and enhancing the efficiency of embedding learning.\n",
        "\n",
        "High-frequency tokens(e.g., \"the\", \"and\")have the higher frequencies and therefore lower probabilites, meaning they are more likely to be skipped. Low-frequency tokens have higher probabilities and more likely to be included."
      ],
      "metadata": {
        "id": "u0atIpDaHLkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample_words(words, threshold):\n",
        "    word_counter = Counter(words)\n",
        "    total = len(words)\n",
        "\n",
        "    def should_discard(word):\n",
        "        frequency = word_counter[word] / total\n",
        "        if frequency > threshold:\n",
        "            p = 1 - np.sqrt(threshold / frequency)\n",
        "            return random.random() < p\n",
        "        return False\n",
        "\n",
        "    return [word for word in words if not should_discard(word)]"
      ],
      "metadata": {
        "id": "c7lxEuL-HHtn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "THRESHOLD_FREQUENCY = 1e-5\n",
        "\n",
        "subsampled_words = subsample_words(words, threshold=THRESHOLD_FREQUENCY)\n",
        "print(\"Original number of words:\", len(words))\n",
        "print(\"Number of words after sub-sampling:\", len(subsampled_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BmGAPvbIeDz",
        "outputId": "70fce45f-82dd-4e16-b8e2-5caedaa941d7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of words: 16561031\n",
            "Number of words after sub-sampling: 4496739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(words[:20])\n",
        "print(subsampled_words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMldv82pPsll",
        "outputId": "e183698e-bf0f-4131-a384-0ce353e6dca4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
            "['anarchism', 'abuse', 'radicals', 'diggers', 'revolution', 'sans', 'revolution', 'pejorative', 'way', 'violent', 'up', 'label', 'defined', 'anarchists', 'anarchism', 'archons', 'ruler', 'chief', 'anarchism', 'rulers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sub-sampling Analysis\n",
        "\n",
        "After applying sub-sampling to reduce the dominance of high-frequency words, it's helpful to compare how many times each word appears before and after sub-sampling The snippet below displays the first 20 words (sorted by their original frequency), along with their original and subsampled counts.\n",
        "\n",
        "This provides a clear demonstration of how sub-sampling removes excessively frequent words while retaining less common (but potentially more informative)\n",
        "words"
      ],
      "metadata": {
        "id": "9GKE4qLkQGGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "\n",
        "counts = Counter(words)\n",
        "subsampled_counts = Counter(subsampled_words)\n",
        "\n",
        "# we'll focus on the first 20 words in the dataset, sorted by original frequency (descending).\n",
        "sample_words = sorted(set(words[:20]), key=lambda w: counts[w], reverse=True)\n",
        "\n",
        "table_data = []\n",
        "for w in sample_words:\n",
        "    original_count = counts[w]\n",
        "    after_count = subsampled_counts.get(w, 0)\n",
        "    table_data.append([w, original_count, after_count])\n",
        "\n",
        "print(tabulate(table_data, headers=[\"Word\", \"Original Count\", \"Subsampled Count\"], tablefmt=\"simple\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wLNy5n0P_Rp",
        "outputId": "e40e4082-be31-4668-ad04-d79ab73fb423"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word          Original Count    Subsampled Count\n",
            "----------  ----------------  ------------------\n",
            "the                  1061396               13047\n",
            "of                    593677                9910\n",
            "a                     325873                7249\n",
            "as                    131815                4610\n",
            "first                  28810                2208\n",
            "used                   22737                1944\n",
            "english                11868                1346\n",
            "early                  10172                1321\n",
            "including               9633                1162\n",
            "against                 8432                1175\n",
            "term                    7219                1070\n",
            "class                   3412                 714\n",
            "working                 2271                 615\n",
            "originated               572                 281\n",
            "abuse                    563                 304\n",
            "anarchism                303                 231\n",
            "radicals                 116                 116\n",
            "diggers                   25                  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Negative Sampling Distribution\n",
        "Negative sampling is a technique introduced in the Word2Vec paper to make training embeddings computationally efficent and effective. Instead of computing the gradients for all words in the vocabulary (which can be computationally expensive, especially for large vocabularies), negative sampling trains the model by updating weights for only a subset of words,spesifically, a small number of \"negative\" (incorrect) samples for each positive (correct) context pair."
      ],
      "metadata": {
        "id": "FZtXuC9qR-VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_negative_sampling_distribution(indexed_words):\n",
        "    counts = np.bincount(indexed_words)\n",
        "    probablility = counts / counts.sum()\n",
        "    probablility_75 = probablility**0.75\n",
        "    return probablility_75 / probablility_75.sum()"
      ],
      "metadata": {
        "id": "fMAHzkBARnqW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Custom Dataset for Skip-Gram Training\n",
        "\n",
        "A PyTorch daataset is implemented to generate training samples for the Skip-Gram model. For each target word, contenxt words within a dynamic window and negative samples are retriebed to train the embeddings efficiently."
      ],
      "metadata": {
        "id": "vm4I76xQUMKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target(words, index, max_window_size=5):\n",
        "    window_size = random.randint(1, max_window_size)\n",
        "    start_position = max(0, index - window_size)\n",
        "    end_position = min(index + window_size + 1, len(words))\n",
        "    return words[start_position:index] + words[index + 1:end_position]"
      ],
      "metadata": {
        "id": "EYtJROHmULeY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_target([i for i in range(20)], 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkSuE2IcVIH6",
        "outputId": "19e163bd-29f1-4e68-e18c-39d82067864c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 4, 6, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_target(subsampled_words[:20], 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYMn37IWVNQo",
        "outputId": "c6aa4b9c-073a-42fc-ed28-a08ba93ca273"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['revolution', 'revolution']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class WordToVecDataset(Dataset):\n",
        "    def __init__(self, indexed_words, window_size=4):\n",
        "        self.indexed_words = indexed_words\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexed_words)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        center_word = self.indexed_words[index]\n",
        "        context_words = get_target(self.indexed_words, index, self.window_size)\n",
        "        return center_word, context_words\n",
        ""
      ],
      "metadata": {
        "id": "77oNsGb0VxL7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WordToVecDataset(\n",
        "    indexed_words=[i for i in range(20)],\n",
        "    window_size=4\n",
        ")\n",
        "\n",
        "center, context = dataset[0]\n",
        "print(center, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOODUTAZWhV6",
        "outputId": "f226185e-f1c8-4c44-9e27-a0e95fb17520"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Collate Function for Efficient Batching\n",
        "\n",
        "This section provides a custom collate function to combine individiual samples into efficient batches for training. It enables parallel processing during model training, significantly accelerating the embedding learning process."
      ],
      "metadata": {
        "id": "0Z3rnmzfYOA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def create_collate_fn(\n",
        "    vocabulary_size,\n",
        "    negative_sampling_distribution,\n",
        "    number_of_negative_samples\n",
        "):\n",
        "  negative_distribution_tensor = torch.tensor(negative_sampling_distribution, dtype = torch.float)\n",
        "\n",
        "  def collate_function(batch):\n",
        "    all_center_words = []\n",
        "    all_context_words = []\n",
        "\n",
        "    #flattten out all center-context pairs\n",
        "    for center_word, context_word_list in batch:\n",
        "      for context_word in context_word_list:\n",
        "        all_center_words.append(center_word)\n",
        "        all_context_words.append(context_word)\n",
        "\n",
        "    center_words_tensor = torch.LongTensor(all_center_words)\n",
        "    context_words_tensor = torch.LongTensor(all_context_words)\n",
        "\n",
        "    # generate negative samples for the entire batch in one shot\n",
        "    total_pairs = len(center_words_tensor)\n",
        "    negative_samples_flat = torch.multinomial(\n",
        "        negative_distribution_tensor,\n",
        "        total_pairs * number_of_negative_samples,\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    negative_samples_tensor = negative_samples_flat.view(total_pairs, number_of_negative_samples)\n",
        "\n",
        "    return center_words_tensor, context_words_tensor, negative_samples_tensor\n",
        "  return collate_function"
      ],
      "metadata": {
        "id": "XiO4gZHKWvXG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = WordToVecDataset(indexed_words = [i for i in range(20)],\n",
        "                           window_size = 4)\n",
        "\n",
        "collect_fn = create_collate_fn(20, np.full(20, 1 / 20, dtype = np.float32), 5)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = 4,\n",
        "    shuffle = True,\n",
        "    num_workers = 2,\n",
        "    collate_fn = collect_fn,\n",
        "    drop_last = True\n",
        ")\n",
        "\n",
        "centers_tensor, contexts_tensor, negatives_tensor = next(iter(dataloader))\n",
        "print(centers_tensor)\n",
        "print(contexts_tensor)\n",
        "print(negatives_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2or6RiRaRl3",
        "outputId": "4a5155e1-d4b6-49b1-f970-c9c724c7bc59"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14, 14, 14, 14, 14, 14, 14, 14, 12, 12, 13, 13, 13, 13, 13, 13,  8,  8,\n",
            "         8,  8,  8,  8,  8,  8])\n",
            "tensor([10, 11, 12, 13, 15, 16, 17, 18, 11, 13, 10, 11, 12, 14, 15, 16,  4,  5,\n",
            "         6,  7,  9, 10, 11, 12])\n",
            "tensor([[ 3,  0, 11,  8, 19],\n",
            "        [19, 13, 16,  3, 17],\n",
            "        [ 5, 17, 19,  0, 14],\n",
            "        [ 6,  8,  1, 16,  0],\n",
            "        [ 2,  3,  2,  3, 13],\n",
            "        [17,  0, 17,  6, 14],\n",
            "        [18,  7,  2, 18, 16],\n",
            "        [18, 15, 17, 12,  8],\n",
            "        [18, 13,  6, 17, 13],\n",
            "        [14, 13,  4, 18, 17],\n",
            "        [16, 12,  7,  4,  4],\n",
            "        [ 9,  7,  9,  6, 12],\n",
            "        [15, 19,  1,  2,  8],\n",
            "        [15,  6, 12,  1, 16],\n",
            "        [18,  5,  8, 18,  0],\n",
            "        [18,  7,  6,  1, 11],\n",
            "        [ 0, 18, 16, 14, 11],\n",
            "        [ 1, 14, 16,  6, 13],\n",
            "        [ 9,  2, 19,  8,  5],\n",
            "        [12,  0, 14, 11,  1],\n",
            "        [18,  9,  5,  8,  1],\n",
            "        [ 7, 15, 10, 10, 16],\n",
            "        [11,  3, 11, 10,  9],\n",
            "        [ 2, 15, 18, 18, 18]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBBN5v7Ra68C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}